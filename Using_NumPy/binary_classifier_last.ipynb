{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9280e740-0300-44fe-8ca9-09c4b4f7c7c7",
      "metadata": {
        "id": "9280e740-0300-44fe-8ca9-09c4b4f7c7c7"
      },
      "source": [
        "### Table of content<a class=\"anchor\" id=\"table\"></a>\n",
        "* [1 - Data](#chapter1)\n",
        "    * [1.1 - Data loading](#section_1_1)\n",
        "    * [1.2 - Handling mini-batches](#section_1_2)\n",
        "* [2 - Pieces of a Neural Network](#chapter2)\n",
        "    * [2.1 - The Forward Propagation](#fwd_prop)\n",
        "    * [2.2 - The Cost function](#cost_fct)\n",
        "    * [2.3 - The Backward Propagation](#backprop)\n",
        "    * [2.4 - Implementing the gradient descent algorithm](#gda)\n",
        "* [3 - Assembling the pieces](#pieces)\n",
        "    * [3.1 - Going forward](#going_fwd)\n",
        "    * [3.2 - Going backward](#going_bwd)\n",
        "    * [3.3 - Initializations](#init)\n",
        "    * [3.4 - Prediction](#prediction)\n",
        "* [4 - Training the model](#training)\n",
        "    * [4.1 - The training function](#training_fct)\n",
        "    * [4.2 - Preprocessing the input features](#preprocess)\n",
        "    * [4.3 - Finally : training the model](#training_fin)\n",
        "    * [4.4 - Discussion](#discussion)\n",
        "* [5 - Improvement](#improv)\n",
        "    * [5.1 - Regularization](#regularization)\n",
        "        * [5.1.1 - L2 Regularization](#L2)\n",
        "        * [5.1.2 - Dropout Regularization](#dropout)\n",
        "    * [5.2 - Optimization](#optim)\n",
        "        * [5.2.1 - Mini-Batch Gradient Descent](#minibatch)\n",
        "        * [5.2.2 - Adding Momentum](#momentum)\n",
        "        * [5.2.3 - Adam optimization](#adam)\n",
        "        * [5.2.4 - Batch Normalization](#BN)\n",
        "* [6 - Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a64db3b2-f832-4f17-8df6-2143bc32d5dc",
      "metadata": {
        "id": "a64db3b2-f832-4f17-8df6-2143bc32d5dc"
      },
      "source": [
        "# NumPy Binary Classifier \n",
        "\n",
        "In this notebook, I would like to show you what is the main logic behind the common deep learning frameworks. I found this interesting since I usually tweet the hyperparameters framework's functions without really knowing what is changing into the equations.\n",
        "\n",
        "So, a matrix point of view gives a better overview of *how a Neural Network works*, and once you're familiar with those notions, you will be able to construct deeper and more complex N.N. with the help of deep learning frameworks, knowing what you are doing. Also, it will help you to tune your model, once it has been trained, to improve it.\n",
        "\n",
        "The goal of this classifier is to detect whether an image is a Pikachu or a Jigglypuff (english version of Rondoudou)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da41b93-c61a-4104-8616-3da572eb0ee9",
      "metadata": {
        "id": "2da41b93-c61a-4104-8616-3da572eb0ee9"
      },
      "source": [
        "## [ 1 - Data](#table) <a class=\"anchor\" id=\"chapter1\"></a>\n",
        "First we need to work on the dataset to feed the neural network with the right dimensions, types etc. For the purpuse of the exercice, I didn't chose a large dataset, it contains :\n",
        "- 98 Pikachu images (label 0)\n",
        "- 76 Jigglypuff [Rondoudou] images (label 1)\n",
        "\n",
        "So 174 images that we have to randomize and split into train and test sets. \n",
        "\n",
        "### [1.1 - Data loading](#table) <a class=\"anchor\" id=\"section_1_1\"></a>\n",
        "We are going to create a function which returns train/val sets and their correspondant labels. This function should let us decide the size of the test set.\n",
        "\n",
        "Note : we usually also construct a *validation* dataset used to tune the trained model (and ensure that the *test* dataset is independant to the data used to build the model). Here, we'll use only a test set, refers through this notebook as a *validation* or a *test* set in the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d8beb104-9992-484c-be1a-01a3b24551aa",
      "metadata": {
        "id": "d8beb104-9992-484c-be1a-01a3b24551aa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import math\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3da9f4ea-64f8-4eda-9f3d-72f7710448e7",
      "metadata": {
        "id": "3da9f4ea-64f8-4eda-9f3d-72f7710448e7"
      },
      "outputs": [],
      "source": [
        "def load_data(val_size:float=0.2, seed:int=None) -> tuple:\n",
        "    \"\"\"\n",
        "    Converts images to arrays and returns them randomized through training and \n",
        "    validation set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    val_size : float, optional\n",
        "        Part of validation set. The default is 0.2.\n",
        "    seed : int, optional\n",
        "        Seed for repeatability. The default is None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data_train : np.array of shape (nb_img, HEIGHT, WIDTH, nb_chans)\n",
        "        Training set.\n",
        "    label_train : np.array of shape (nb_img, 1)\n",
        "        Labels of the training set.\n",
        "    data_val : np.array of shape (nb_img, HEIGHT, WIDTH, nb_chans)\n",
        "        Validation set.\n",
        "    label_val : np.array of shape (m, 1)\n",
        "        Labels of the validation set.\n",
        "    classes : np.array of shape (2,)\n",
        "        Classe names : Pikachu / Rondoudou. They are encode in bytes.\n",
        "\n",
        "    \"\"\"\n",
        "    list_pikachu = glob.glob('data/pikachu/*')\n",
        "    list_rondoudou = glob.glob('data/rondoudou/*')\n",
        "    \n",
        "    HEIGHT = 100\n",
        "    WIDTH = 100\n",
        "    CHANNEL = 3\n",
        "    \n",
        "    print(f\"Length pikachu dataset : {len(list_pikachu)}\")\n",
        "    print(f\"Length Rondoudou dataset : {len(list_rondoudou)}\")\n",
        "\n",
        "    classes = np.array([b'Pikachu', b'Rondoudou'])\n",
        "    \n",
        "    # Initialisations\n",
        "    size_dataset = len(list_pikachu) + len(list_rondoudou)\n",
        "    dataset_arr = np.zeros((size_dataset, HEIGHT, WIDTH, CHANNEL))\n",
        "    label = np.zeros((size_dataset, 1), dtype='int')\n",
        "    \n",
        "    # Generating a Pikachu array-type dataset\n",
        "    for k in range(len(list_pikachu)):\n",
        "        with Image.open(list_pikachu[k]) as im :\n",
        "            im = im.resize((HEIGHT, WIDTH), resample=Image.BICUBIC)\n",
        "            im = im.convert(\"RGB\")\n",
        "        img_arr = np.array(im)\n",
        "        dataset_arr[k] = img_arr\n",
        "        \n",
        "    # Generating a Rondoudou array type dataset\n",
        "    i=0\n",
        "    for k in range(len(list_pikachu), len(dataset_arr)):\n",
        "        with Image.open(list_rondoudou[i]) as im2 :\n",
        "            im2 = im2.resize((HEIGHT, WIDTH), resample=Image.BICUBIC)\n",
        "            im2 = im2.convert(\"RGB\")\n",
        "        img_arr = np.array(im2)\n",
        "        dataset_arr[k] = img_arr\n",
        "        label[k] = 1\n",
        "        i+=1\n",
        "    \n",
        "    # Randomizing\n",
        "    n_samples = dataset_arr.shape[0]\n",
        "    n_val = int(val_size * n_samples)\n",
        "    shuffled_indices = np.random.permutation(n_samples)\n",
        "    train_indices = shuffled_indices[:-n_val] \n",
        "    val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "    data_train = dataset_arr[train_indices]\n",
        "    label_train = label[train_indices]\n",
        "    \n",
        "    data_val = dataset_arr[val_indices]\n",
        "    label_val = label[val_indices]\n",
        "    \n",
        "    return data_train, label_train, data_val, label_val, classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "155d5d12-d641-49e2-9994-ee7872d1bb7c",
      "metadata": {
        "id": "155d5d12-d641-49e2-9994-ee7872d1bb7c"
      },
      "source": [
        "### [1.2 - Handling mini-batches](#table) <a class=\"anchor\" id=\"section_1_2\"></a>\n",
        "Using mini-batches is an optimization technique and permits to let gradient descent makes progress *before* finishing of processing the *entire* training set. We need to create a function capable of retriving mini-batches of size `mini_batch_size` with the corresponding labels for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "88cbde7f-06cc-4dfc-a694-aef6295db09a",
      "metadata": {
        "id": "88cbde7f-06cc-4dfc-a694-aef6295db09a"
      },
      "outputs": [],
      "source": [
        "def random_mini_batches(X:np.ndarray, Y:np.ndarray, mini_batch_size:int=64, seed:int=None) -> list:\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.array\n",
        "        Input data, of shape (input size, number of examples)\n",
        "    Y : np.array\n",
        "        True \"label\" vector, of shape (1, number of examples)\n",
        "    mini_batch_size : int, optional\n",
        "        Size of the mini-batches. The default is 64\n",
        "    seed : int, optional\n",
        "        Seed for repeatability. The default is None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mini_batches : list\n",
        "        List of (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    m = X.shape[1]\n",
        "    mini_batches = []\n",
        "    end_minibatch = 0\n",
        "    \n",
        "    # Shuffling X, Y\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
        "    \n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        start_minibatch = k * mini_batch_size\n",
        "        end_minibatch = (k+1) * mini_batch_size\n",
        "        mini_batch_X = shuffled_X[:, start_minibatch : end_minibatch]\n",
        "        mini_batch_Y = shuffled_Y[:, start_minibatch : end_minibatch]\n",
        "        \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "        \n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, end_minibatch : ]\n",
        "        mini_batch_Y = shuffled_Y[:, end_minibatch : ]\n",
        "    \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000b8445-e79e-48fa-a08f-773ad4c11a2c",
      "metadata": {
        "id": "000b8445-e79e-48fa-a08f-773ad4c11a2c"
      },
      "source": [
        "## [2 - Pieces of a Neural Network](#table) <a class=\"anchor\" id=\"chapter2\"></a>\n",
        "In this section, we'll develop functions that are usefull to construct a neural network of size `L` with any number of units for each layer.\n",
        "\n",
        "### [2.1 - The Forward Propagation](#table) <a class=\"anchor\" id=\"fwd_prop\"></a>\n",
        "A neural network adjusts each hidden-layer units weights dependantly to the previous layer and go from the **input layer** to the last **output layer**. This step is called the **forward pass**. Each layer has those following variables of interest :\n",
        "- $W^{[l]}$ the weights of the $l^{th}$ layer\n",
        "- $b^{[l]}$ the bias of the $l^{[th]}$ layer\n",
        "- $Z^{[l]}$ the pre-activation value of the $l^{[th]}$ layer : it results of a linear calculus and depends of $W^{[l]}$, $b^{[l]}$, and $A^{[l-1]}$. Note that $A^{[l-1]}$ is actually the features $X$ for the first layer. \n",
        "- $A^{[l]}$ the activation value of the $l^{[th]}$ layer computes from a given activation function.\n",
        "\n",
        "\n",
        "At layer `l`, each neuron is composed of two parts : a linear part which gives `z`, and an activation part to \"activate\" the neurone, which gives `a`. Using matrix operations to reprensent all the units for a given layer, we got the equations below :\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "      Z^{[l]} = W^{[l]}A^{[l-1]} + b\\\\\n",
        "      A^{[l]} = g(Z^{[l]})\n",
        "    \\end{cases}\\,.\n",
        "\\end{equation}\n",
        "\n",
        "Note :\n",
        "- We use lower case to talk about a neuron (or a unit) and capital case to represent a layer of neurons.\n",
        "- `g` represents a given activation function for this neuron (e.g. sigmoid activation function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a9e25a1d-920d-465f-b86b-93d72b903187",
      "metadata": {
        "id": "a9e25a1d-920d-465f-b86b-93d72b903187"
      },
      "outputs": [],
      "source": [
        "def activation_function(Z:np.ndarray, activation:str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the activation function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Z : np.array\n",
        "        pre-activation parameter.\n",
        "    activation : str\n",
        "        Name of the activation function (sigmoid or relu)\n",
        "    Returns\n",
        "    -------\n",
        "    A : np.array of shape Z.shape\n",
        "        Post-activation parameter\n",
        "\n",
        "    \"\"\"\n",
        "    A = np.zeros(Z.shape)\n",
        "    if activation == 'sigmoid':\n",
        "        A = 1 / (1 + np.exp(-Z))\n",
        "    elif activation == 'relu':\n",
        "        A = np.maximum(0,Z) \n",
        "    return A\n",
        "\n",
        "\n",
        "def forward_pass(A_prev:np.ndarray, W:np.ndarray, b:np.ndarray, activation:str) -> tuple:\n",
        "    \"\"\"\n",
        "    Implement the forward pass\n",
        "    LINEAR->ACTIVATION layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : np.array of shape (n[l-1], m)\n",
        "        Activations from previous layer (or input data).\n",
        "    W : np.array of shape (n[l], n[l-1])\n",
        "        Weights matrix.\n",
        "    b : np.array of size (n[l], 1)\n",
        "        Bias vector.\n",
        "    activation : str\n",
        "        Name of activation function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A : np.array\n",
        "        Post-activation value.\n",
        "    cache : tuple\n",
        "        containing \"linear_cache\" and \"activation_cache\". Storing variable for \n",
        "        computing the backward pass efficiently.\n",
        "\n",
        "    \"\"\"\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    A = activation_function(Z, activation)\n",
        "\n",
        "    linear_cache = (A_prev, W, b)\n",
        "    activation_cache = Z\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1847a062",
      "metadata": {
        "id": "1847a062"
      },
      "source": [
        "### [2.2 - The Cost function](#table) <a class=\"anchor\" id=\"cost_fct\"></a>\n",
        "The cost function computes how much the predicted result (at the output layer) is far from a given ground truth. We'll define *cost function* `J` as the sum of the loss functions `L` over all the examples (from 1 to m). For logistic regression, we usually use the *cross entropy cost function* (defined above). \n",
        "\n",
        "In machine learning, we want to *minimise* the cost function *i.e.* find the best configuration of weights en bias who provide a prediction that makes the cost function tends toward zero. A well known algorithm to optimize parameters is the *gradient descent algorithm* : it computes new weights and bias for each iteration based on how much the cost function `J` varies and it is scaled by a `learning rate` $\\alpha$. \n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "    W = W - \\alpha \\frac{\\partial J}{\\partial W} \\\\\n",
        "    b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
        "    \\end{cases}\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    J(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{Y}, Y) =  \\frac{-1}{m}\\sum_{i=1}^{m}\\left(Y\\log(\\hat{Y}) + (1-Y)\\log(1-\\hat{Y})\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "*Note : since the gradient descent is applied only on a portion of the data when using mini-batch, we do not normalized by the number of examples ($\\frac{1}{m}$ factor) when dealing with mini-batches.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c2b8b90a",
      "metadata": {
        "id": "c2b8b90a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_cost(yhat:np.ndarray, Y:np.ndarray, mini_batch_size:int=None) -> float:\n",
        "    \"\"\"\n",
        "    Compute the cost function, sum of the loss function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    yhat : np.array of shape (1, m)\n",
        "        Probability vector corresponding to the label predictions. It's actually\n",
        "        the activation at the layer L : AL.\n",
        "    Y : np.array of shape (1, m)\n",
        "        True label vector.\n",
        "    mini_batch_size : int, optionnal\n",
        "        Arg used to trigger the normalization of the cost by 1/m if there is no\n",
        "        mini_batches. Default is None.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cost : float\n",
        "        Cross-entropy cost function with or without dividing by number of \n",
        "        training examples\n",
        "\n",
        "    \"\"\"\n",
        "    AL = yhat\n",
        "    cost = np.sum(-np.log(AL)*Y - np.log(1-AL)*(1-Y))\n",
        "    \n",
        "    if mini_batch_size is None:\n",
        "        m = Y.shape[1]\n",
        "        cost = (1/m) * cost\n",
        "\n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2688d84c-8b37-4587-99fb-b6c1380051bd",
      "metadata": {
        "id": "2688d84c-8b37-4587-99fb-b6c1380051bd"
      },
      "source": [
        "### [2.3 - The Back Propagation](#table) <a class=\"anchor\" id=\"backprop\"></a>\n",
        "We saw earlier how the gradient descent algorithm works : \\\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "    W = W - \\alpha \\frac{\\partial J}{\\partial W} \\\\\n",
        "    b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
        "    \\end{cases}\\,.\n",
        "\\end{equation}\n",
        "\n",
        "The goal of the back propagation is to calculate the gradients of the cost function with respect to the parameters. Knowing the formula of $J$, we can understand that it will be a little bit tricky to get those gradients. \n",
        "\n",
        "We recall that $\\hat{Y}$ is the result of the forward pass which involved one linear part $Z^{[l]}(W^{[l]},A^{[l-1]}, b)$ and a non-linear part $A^{[l]}(Z^{[l]})$. So, $J$ is a function of functions. I won't go into calculus details, but if you do the maths, you'll get to these formulas :\n",
        "\n",
        "$$ dZ^{[l]} = \\frac{\\partial J }{\\partial Z^{[l]}} = dA^{[l]} g'(Z^{[l]}) $$\n",
        "$$ dW^{[l]} = \\frac{\\partial J }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
        "$$ db^{[l]} = \\frac{\\partial J }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l]}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial J }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7ed5bb8c-4237-48be-aebb-c88321044d96",
      "metadata": {
        "id": "7ed5bb8c-4237-48be-aebb-c88321044d96"
      },
      "outputs": [],
      "source": [
        "def backward_activation_function(dA:np.ndarray, cache:np.ndarray, activation:str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Implements the backward propagation for SIGMOID or ReLU. \n",
        "    Compute dZ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dA : np.array of any shape\n",
        "        Post-activation gradient\n",
        "    cache : np.array of shape of dA\n",
        "        Used to compute backward propagation efficiently.\n",
        "    activation : str\n",
        "        Name of the activation function\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dZ : np.array of shape dA.shape\n",
        "        Gradient of the cost with respect to Z.\n",
        "\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    dZ = np.zeros(Z.shape)\n",
        "    if activation == 'sigmoid':\n",
        "        s = activation_function(Z, activation)\n",
        "        dZ = dA * s * (1 - s)\n",
        "    elif activation == 'relu':\n",
        "        dZ = np.array(dA, copy=True)\n",
        "        dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def backward_pass(dA:np.ndarray, cache:tuple, activation:str) -> tuple:\n",
        "    \"\"\"\n",
        "    Compute the backward pass :\n",
        "    LINEAR -> dZ -> GRADS\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dA : np.array\n",
        "         Post-activation gradient for current layer l.\n",
        "    cache : tuple\n",
        "        containing \"linear_cache\" and \"activation_cache\". These caches come from \n",
        "        the forward pass.\n",
        "    activation : str\n",
        "        Activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dA_prev : np.array of shape A.shape\n",
        "        Gradient of the cost with respect to the activation (previous layer l-1)\n",
        "    dW : np.array of shape W.shape\n",
        "        Gradient of the cost with respect to the activation (current layer)\n",
        "    db : np.array of shape b.shape\n",
        "        Gradient of the cost with respect to b (current layer l).\n",
        "\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    A_prev, W, b = linear_cache\n",
        "    m = A_prev.shape[1]\n",
        "    dZ = np.zeros(dA.shape)\n",
        "    \n",
        "    if activation == 'relu':\n",
        "        dZ = backward_activation_function(dA, activation_cache, 'relu')\n",
        "    elif activation == 'sigmoid':\n",
        "        dZ = backward_activation_function(dA, activation_cache, 'sigmoid')\n",
        "    \n",
        "    ### Compute grads\n",
        "    dW = (1./m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1./m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev =  np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1b0fb0-fcec-4194-a02b-d9919560a638",
      "metadata": {
        "id": "0f1b0fb0-fcec-4194-a02b-d9919560a638"
      },
      "source": [
        "### [2.4 - Implementing the gradient descent algorithm](#table) <a class=\"anchor\" id=\"gda\"></a>\n",
        "Let's built a function that updates parameters with the corresponding gradient at a rate set by the `learning_rate` hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "af0e34c5-eb21-4ea7-9470-cc832ad83450",
      "metadata": {
        "id": "af0e34c5-eb21-4ea7-9470-cc832ad83450"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters:dict, grads:dict, learning_rate:float) -> dict:\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : dict\n",
        "        Containing the parameters.\n",
        "    grads : dict\n",
        "        Contain the grads output of model_backward.\n",
        "    learning_rate : float\n",
        "        Tune the rate of learning.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    parameters : dict\n",
        "        Contain the update parameters.\n",
        "\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2\n",
        "    \n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
        "            learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
        "            learning_rate * grads[\"db\" + str(l+1)]\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7337fe",
      "metadata": {
        "id": "0e7337fe"
      },
      "source": [
        "## [3 - Assembling the pieces](#table) <a class=\"anchor\" id=\"pieces\"></a>\n",
        "Until now, we've build functions that will help us to proceed to the forward pass and the backward pass for any layers. Let's now assemble some pieces.\n",
        "\n",
        "### [3.1 - Going forward](#table) <a class=\"anchor\" id=\"going_fwd\"></a>\n",
        "The function below computes the forward pass through the layers from the first to the output layer. For network stability, we often use the **ReLU** activation function to activate the neurons *inside* the network and then, prevent gradients to vanish. Since we need a binary output as a probability of prediction, the output layer will be activated by a **sigmoid** activation function. \n",
        "\n",
        "*Note : the hyperparameter `keep_prob` is used to regularize the model by **Dropout**, see the [Dropout Regularization](#dropout) part of this notebook.* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5f92abbc",
      "metadata": {
        "id": "5f92abbc"
      },
      "outputs": [],
      "source": [
        "def forward(X:np.ndarray, parameters:dict, keep_prob:float=1) -> tuple :\n",
        "    \"\"\"\n",
        "    Implement forward propagation for a single layer (layer l) with ReLU activation function inside\n",
        "    the network and Sigmoid activation function for the output layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.array\n",
        "        Data input.\n",
        "    parameters : dict\n",
        "        Parameters of the current layer\n",
        "    keep_prob : float, optional\n",
        "        Dropout factor : trigger the dropout regularization if < 1. The default is 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    AL : np.array\n",
        "        Activation value from the output (last) layer, i.e. prediction vector, also called yhat.\n",
        "    caches : list\n",
        "        Contain every cache of the linear part of the forward propagation : there are L of them, \n",
        "        indexed from 1 to L.\n",
        "    listD : list\n",
        "        list containing every mask used to dropout the layer [l] : there are L-1 of them, \n",
        "        indexed from 1 to L-1.\n",
        "\n",
        "    \"\"\"\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "    listD = []\n",
        "        \n",
        "    ### Hidden Layers\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        W = parameters[\"W\"+str(l)]\n",
        "        b = parameters[\"b\"+str(l)]\n",
        "        A, cache = forward_pass(A_prev, W, b, activation=\"relu\")\n",
        "        caches.append(cache)\n",
        "        \n",
        "        # Handle dropout regularization\n",
        "        if keep_prob < 1:\n",
        "            D = np.random.rand(A.shape[0], A.shape[1])\n",
        "            D = (D<keep_prob).astype(int) # mask creation\n",
        "            A = A * D\n",
        "            A = A / keep_prob\n",
        "            listD.append(D)\n",
        "    \n",
        "    ### Output layer\n",
        "    A_prevL = A\n",
        "    l = L-1\n",
        "    WL = parameters[\"W\"+str(l+1)]\n",
        "    bL = parameters[\"b\"+str(l+1)]\n",
        "    AL, cache = forward_pass(A_prevL, WL, bL, activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    return AL, caches, listD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e6440d",
      "metadata": {
        "id": "74e6440d"
      },
      "source": [
        "### [3.2 - Going backward](#table) <a class=\"anchor\" id=\"going_bwd\"></a>\n",
        "The function below computes the backward pass through the layers from the output layer to the first hidden layer. \n",
        "So, like a reverse of the forward pass, we start to compute the gradients of the sigmoid activation function at the output layer (layer L) and then going backward until the first hidden layer : from layer L-1 to layer 1, using $dA^{[L-1]}$ to get $dA^{[L-2]}$... until getting $dA^{[0]}$.\n",
        "\n",
        "- *Note : again, the hyperparameter `keep_prob` and the parameter `listD` are used to regularize the model by **Dropout Regularization**, see the [Dropout Regularization](#dropout) part of this notebook.* \n",
        "- *Note 2 : the hyperparameter `lambd` is used to regularize the model by **L2 Regularization**, see the [L2 Regularization](#L2) part of this notebook.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e74771e4",
      "metadata": {
        "id": "e74771e4"
      },
      "outputs": [],
      "source": [
        "def backward(AL:np.ndarray, Y:np.ndarray, caches:list, lambd:float=0, listD:list=[], keep_prob:float=1.) -> dict:\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the \n",
        "    [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group.\n",
        "    Can use the L2 and/or dropout regularizations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    AL : np.array of shape Y.shape\n",
        "        Probability vector, output of the forward propagation (L_model_forward()).\n",
        "        Also called yhat.\n",
        "    Y : np.array\n",
        "        True \"label\" vector (containing 0 or 1).\n",
        "    caches : list\n",
        "        Contain: every cache of linear_activation_forward() with \"relu\" \n",
        "        (it's caches[l], for l in range(L-1) i.e l = 0...L-2) and the cache \n",
        "         of linear_activation_forward() with \"sigmoid\" (it's caches[L-1]).\n",
        "    lambd : float\n",
        "        L2 regularization factor : trigger the L2reg if > 0. The default is 0.\n",
        "    listD : list\n",
        "        list containing every mask used to dropout the layer [l]. \n",
        "        (there are L-1 of them, indexed from 1 to L-1).\n",
        "    keep_prob : float, optional\n",
        "        Dropout factor : trigger the dropout regularization if < 1. The default is 1.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    grads : dict\n",
        "        A dictionary with the gradients.\n",
        "\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    \n",
        "    dAL =  - np.divide(Y, AL) + np.divide(1 - Y, 1 - AL)\n",
        "    \n",
        "    ### Output layer L (using dAL to get dAprev i.e. dA[L-1])\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp, dW_temp, db_temp = backward_pass(dAL, current_cache, 'sigmoid')\n",
        "    \n",
        "    ### L2 regularization terms initialization\n",
        "    L2reg_dW = np.array([0.])\n",
        "    L2reg_db = np.array([0.])\n",
        "    \n",
        "    ### Handle L2 regularization for output layer\n",
        "    if lambd > 0 :\n",
        "        L2reg_dW = (lambd/m) * dW_temp\n",
        "        L2reg_db = (lambd/m) * db_temp\n",
        "    \n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp + L2reg_dW\n",
        "    grads[\"db\" + str(L)] = db_temp + L2reg_db\n",
        "    \n",
        "    ### Hidden layers : from layer L-1 to layer 1 using dA[L-1] to get dA[L-2]... until dA[0]\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = backward_pass(grads[\"dA\" + str(l+1)], current_cache, 'relu')\n",
        "        \n",
        "        # Handle dropout \n",
        "        if keep_prob < 1:\n",
        "            dA_prev_temp = dA_prev_temp * listD[l]\n",
        "            dA_prev_temp = dA_prev_temp / keep_prob\n",
        "        \n",
        "        # Handle L2 regularization for hidden layers\n",
        "        if lambd > 0:\n",
        "            L2reg_dW = (lambd/m) * dW_temp\n",
        "            L2reg_db = (lambd/m) * db_temp\n",
        "            \n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp + L2reg_dW\n",
        "        grads[\"db\" + str(l + 1)] = db_temp + L2reg_db\n",
        "        \n",
        "    return grads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b889f9c",
      "metadata": {
        "id": "1b889f9c"
      },
      "source": [
        "### [3.3 - Initializations](#table) <a class=\"anchor\" id=\"init\"></a>\n",
        "The first layer's weights are initialized randomly to \"initiate the learning\". Here we'll use the *He-et-al Initialization* technique since we are using ReLU activation. This method initiates weights randomly and scales them regarding the size of the previous layer :\n",
        "\n",
        "$$W^{[0]} = \\frac{W_{rand}}{\\sqrt{[l^{-1}]}}$$\n",
        "\n",
        "Where :\n",
        "- $W_{rand}$ is a randomize matrice of weights\n",
        "- $[l^{-1}]$ is the size of the layer $l^{-1}$\n",
        "\n",
        "*Note : one could demonstrates that initialized the biais b randomly is useless and doesn't impact the learning of the network. So usually, it is sets to zero with the right dimensions.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "45578993-c453-4c59-abb6-37e5bde8cae5",
      "metadata": {
        "id": "45578993-c453-4c59-abb6-37e5bde8cae5"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_dims:list, seed:int=None) -> dict:\n",
        "    \"\"\"\n",
        "    Initialize randomly parameters W and b of each layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_dims : list[np.array]\n",
        "        Containing the dimensions of each layer in the network.\n",
        "    seed : int, optional\n",
        "        Seed for repeatability. The default is None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    parameters : dict\n",
        "        Contain parameters \"W1\", \"b1\", ..., \"WL\", \"bL\".\n",
        "\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
        "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l],1))\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d91cb584",
      "metadata": {
        "id": "d91cb584"
      },
      "source": [
        "### [3.4 - Prediction](#table) <a class=\"anchor\" id=\"prediction\"></a>\n",
        "\n",
        "The prediction function is used to get a prediction when it has been decided that the parameter's model are well trained, *i.e* that the cost function has decreased enough. This function is just the forward part of the model, using the optimized parameters `W`and `b` get in the training part, and converts the probability prediction into binary label 0 or 1. It gives also the **accuracy** of the model as the sum of the predicting labels regarding the true labels :\n",
        "\n",
        "$$ accuracy = \\frac{1}{m}\\sum^{m}_{1}(\\hat{y} == y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "55f066e9",
      "metadata": {
        "id": "55f066e9"
      },
      "outputs": [],
      "source": [
        "def predict(X:np.ndarray, y:np.ndarray, parameters:dict, training_set:bool=True) -> float:\n",
        "    \"\"\"\n",
        "    Function used to predict the results of a  L-layer neural network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.array\n",
        "        Input dataset.\n",
        "    y : np.array\n",
        "        True \"label\" vector (containing 0 or 1).\n",
        "    parameters : dict\n",
        "        Parameters of the trained model.\n",
        "    training_set : bool, optional\n",
        "        Verbose parameter\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    p : int\n",
        "        Prediction label (0 or 1).\n",
        "\n",
        "    \"\"\"\n",
        "    if training_set : \n",
        "        name = \"Training\"\n",
        "    else:\n",
        "        name = \"Validation\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward pass\n",
        "    probas, caches, _ = forward(X, parameters)\n",
        "        \n",
        "    for i in range(0, probas.shape[1]):\n",
        "        p[0,i] = round(probas[0,i])\n",
        "        \n",
        "    print(f\"{name} Accuracy: {np.sum((p == y)/m)*100 :.2f}%\")\n",
        "    return np.sum((p == y)/m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b73f1e",
      "metadata": {
        "id": "68b73f1e"
      },
      "source": [
        "## [4 - Training the model](#table) <a class=\"anchor\" id=\"training\"></a>\n",
        "### [4.1 - The training function](#table)<a class=\"anchor\" id=\"training_fct\"></a>\n",
        "\n",
        "\n",
        "Now that all the functions are well defined, we can built our model using the forward pass, the backward pass etc...\n",
        "\n",
        "Let's create the `NN_model` function with the right arguments that will be used to train the parameters `W` and `b` through `epochs` representing one whole step of calculation (forward + backward + parameters update + loss computation). \n",
        "\n",
        "As explained earlier, the training will handle the `mini-batch` optimization technique so, two loops will be necessary : one to go through the epochs and one to analyse each set of image stored as mini-batch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "e4d6190d",
      "metadata": {
        "id": "e4d6190d"
      },
      "outputs": [],
      "source": [
        "def NN_model(X:np.ndarray, Y:np.ndarray, layers_dims:list, learning_rate:float=0.0075, \n",
        "             mini_batch_size:int=64, epochs:int=1000) -> tuple:\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    With potential regularization techniques and optimization techniques.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.array\n",
        "        Input features.\n",
        "    Y : np.array of shape (1, m)\n",
        "        True \"label\" vector (containing 0 if Pikachu, 1 if Rondoudou)\n",
        "    layers_dims : list of length (number of layers + 1)\n",
        "        Contain the input size and each layer size.\n",
        "    optimizer : str, optional\n",
        "        Optimization technique to use. The default is gradient descent 'gd'.\n",
        "    learning_rate : float, optional\n",
        "        Learning rate of the gradient descent update rule. The default is 0.0075.\n",
        "    mini_batch_size : int, optional\n",
        "        Size of the mini_batch.\n",
        "    epochs : int, optional\n",
        "         Number of iterations of the optimization loop. The default is 1000.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    parameters : dict\n",
        "        Parameters learnt by the model.\n",
        "    costs : list\n",
        "        List of costs for each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    costs = []\n",
        "    t = 0\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(layers_dims, seed=42)\n",
        "    \n",
        "    \n",
        "    print(f\"Training is starting with minibatch size = {mini_batch_size} ...\")\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Chose a minibatch per epoch\n",
        "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed=42)\n",
        "        cost_total = 0\n",
        "        \n",
        "        # Training through minibatch\n",
        "        for minibatch in minibatches:\n",
        "            # Select a minibatch\n",
        "            (minibatch_X, minibatch_Y) = minibatch \n",
        "        \n",
        "            # Forward pass\n",
        "            yhat, caches, listD = forward(minibatch_X, parameters)\n",
        "            \n",
        "            # Compute cost and add to the cost total\n",
        "            cost = compute_cost(yhat, minibatch_Y, mini_batch_size)\n",
        "            cost_total += cost\n",
        "        \n",
        "            # Backward pass\n",
        "            grads = backward(yhat, minibatch_Y, caches)\n",
        "        \n",
        "            # Update parameters\n",
        "            parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Compute average cost\n",
        "        cost_avg = cost_total / m\n",
        "            \n",
        "        # Printing\n",
        "        if epoch==1 or epoch % 100 == 0 or epoch == epochs: #- 1:\n",
        "            print(\"{} ------- Cost after epoch {}: {:.4f}\".format(\n",
        "                datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S'), \n",
        "                epoch, \n",
        "                cost_avg))\n",
        "        if epoch % 100 == 0 or epoch == epochs:\n",
        "            costs.append(cost_avg)\n",
        "            \n",
        "    return parameters, costs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8935a2ec",
      "metadata": {
        "id": "8935a2ec"
      },
      "source": [
        "### [4.2 - Preprocessing the input features](#table)<a class=\"anchor\" id=\"preprocess\"></a>\n",
        "\n",
        "Let's use our `load_data` and `random_mini_batches` functions to create input features. Note that to get a faster learning, it's common use to normalize the data between 0 and 1. We'll also perform some reshapes to fit the data to the required model's input shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "85b4fe47",
      "metadata": {
        "id": "85b4fe47"
      },
      "outputs": [],
      "source": [
        "# Loading libraries\n",
        "import numpy as np\n",
        "import datetime\n",
        "import glob\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "30f3b85b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30f3b85b",
        "outputId": "8cead9d9-e06c-4e8f-c908-4bdece3354e0",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length pikachu dataset : 98\n",
            "Length Rondoudou dataset : 76\n",
            "\n",
            "----------------------\n",
            "\n",
            "data_train's shape: (30000, 140)\n",
            "data_val's shape: (30000, 34)\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing\n",
        "data_train_orig, label_train, data_val_orig, label_val, classes = load_data(seed=42)\n",
        "\n",
        "m_train = data_train_orig.shape[0]\n",
        "m_test = data_val_orig.shape[0]\n",
        "\n",
        "# Reshape the training and test examples \n",
        "data_train_flatten = data_train_orig.reshape(m_train, -1).T\n",
        "data_val_flatten = data_val_orig.reshape(m_test, -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "data_train = data_train_flatten/255.\n",
        "data_val = data_val_flatten/255.\n",
        "\n",
        "label_train = label_train.T\n",
        "label_val = label_val.T\n",
        "\n",
        "print (\"\\n----------------------\\n\")\n",
        "print (\"data_train's shape: \" + str(data_train.shape))\n",
        "print (\"data_val's shape: \" + str(data_val.shape))\n",
        "print (\"\\n----------------------\\n\")\n",
        "\n",
        "n_x = data_train_orig.shape[1]*data_train_orig.shape[2]*data_train_orig.shape[3]\n",
        "n_y = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8503970f",
      "metadata": {
        "id": "8503970f"
      },
      "source": [
        "### [4.3 - Finally : training the model](#table)<a class=\"anchor\" id=\"training_fin\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "f9365a3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9365a3b",
        "outputId": "dc4f80f8-b10c-4c23-b9e4-c48d547bab61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is starting with minibatch size = 64 ...\n",
            "2022-04-25_08.48.12 ------- Cost after epoch 1: 0.6921\n",
            "2022-04-25_08.48.16 ------- Cost after epoch 100: 0.5510\n",
            "2022-04-25_08.48.21 ------- Cost after epoch 200: 0.4131\n",
            "2022-04-25_08.48.25 ------- Cost after epoch 300: 0.3640\n",
            "2022-04-25_08.48.30 ------- Cost after epoch 400: 0.3377\n",
            "Training Accuracy: 97.86%\n"
          ]
        }
      ],
      "source": [
        "# Training a 4-layers model\n",
        "layers_dims = [n_x, 10, 5, 3, n_y]\n",
        "parameters, costs = NN_model(data_train, \n",
        "                             label_train, \n",
        "                             layers_dims, \n",
        "                             learning_rate = 0.001,\n",
        "                             epochs = 400)\n",
        "pred_train = predict(data_train, label_train, parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5193b8",
      "metadata": {
        "id": "7d5193b8"
      },
      "source": [
        "### [4.4 - Discussion](#table)<a class=\"anchor\" id=\"discussion\"></a>\n",
        "Yeay ! The model works and it learns ! \n",
        "We have walked a long way to come here, defined functions to build our own deep neural network from scratch. Now let's talk about the results.\n",
        "\n",
        "First, we can see that the training accuracy of this simple network is high. As we now, the training accuracy is a measure of how the network behaves facing knowing images. Let's see how it behaves with unknowing images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "1757dd8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1757dd8f",
        "outputId": "4547bc5d-478c-4810-c941-a4fc0f2b667b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 94.12%\n"
          ]
        }
      ],
      "source": [
        "pred_test = predict(data_val, label_val, parameters, training_set=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de6547a",
      "metadata": {
        "id": "0de6547a"
      },
      "source": [
        "Well, 94% accuracy, for a start it's interesting !\n",
        "\n",
        "Unfortunately, things might not always be so easy : first, remember that a high training accuracy regarding validation (or test) accuracy is a strong alert of overfitting. That is, the network will learn the data too \"strongly\" and focuses on all the noisy and unhelpful parts, it's often the sign of an unflexible too complex model (it is too constraint to generalize well to unknown images). In that case, we talk about **overfitting** or a model with high **variance**.\n",
        "\n",
        "Here, the model seems to generelize well to unknown images (at least, regarding the training accuracy). But relative to this easy task (cartoon images, small dataset) it should give better results (near 99%). We've noticed that the model's training is highly dependant of the random initialization and could render better (or worst) results (we set a seed here to bypass this effect). Also, we might be able to get better results by building a more complex model with more units (neurons) in each layer.\n",
        "\n",
        "\n",
        "**Finaly :** \n",
        "\n",
        "One thing we should remember is the size of our database : it's a really small database for image recognition. The training set is so small that it isn't representative of the various Pikachu or Rondoudou images that could be found on the web. So, we have to beware of the robustness of our model (which is really weak here). \\\n",
        "Also, we could talk about the kind of image that we want to learn : Pikachus or Rondoudous are actually drawings, cartoon objects. So, we expect the images to be net, with small textures and colors (like hair, beard, skin color, clothing etc). Knowing that, we are expecting the network to learn more easily and so required less input images for training. \\\n",
        "And last but not least, this model uses what we call \"fully connected layers\" : each pixel of the image are fully linked to each layer of the network by a linear function (before non-linear activation). This, leads to computationaly intensive work and a weak spatial invariance (capacity to recognize a target as the target, even when its appearance varies in some way like a translation, light variations...). Convolutional Neural Networks will fix this issues by \"highlighting\" region of interest in the image and sharing parameters through the network (but this feature is beyond the scope of this notebook).\n",
        "\n",
        "**What happends next ?** \\\n",
        "We saw that the size of our dataset and the complexity of our model don't need tricks to speed up the learning, neither to improve it. Nevertheless, in the case of large datasets, with maybe bigger images and more complex models, tricks need to be invoke to ensure the model is generelizing well and the loss function is decreasing well enough in a fair descent running time. So, let's see some improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee1aeb8",
      "metadata": {
        "id": "5ee1aeb8"
      },
      "source": [
        "## [5 - Improvement](#table) <a class=\"anchor\" id=\"improv\"></a>\n",
        "In this section, we'll show the bases of the improvements that could be implement in the network to get a better control of the learning process. We will show two ways of such improvement : regularization techniques, who's going to make the network simpler and then, prevent overfitting. Optimization techniques, speeding up the learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0zIua7TJQhpq",
      "metadata": {
        "id": "0zIua7TJQhpq"
      },
      "source": [
        "### [5.1 - Regularizations](#table) <a class=\"anchor\" id=\"regularization\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w6eqRaQNQEyg",
      "metadata": {
        "id": "w6eqRaQNQEyg"
      },
      "source": [
        "#### [5.1.1 - L2 Regularization](#table) <a class=\"anchor\" id=\"L2\"></a>\n",
        "\n",
        "L2 regularization technique or *the technique of the weight decays* introduces a `lambd` hyperparameter to control the weights of the matrices (see the [forward](#going_fwd) and [backward](#going_bwd) parts). It also adds a term in the cost formula such as :\n",
        "\\begin{equation}\n",
        "    {cost}_{L2reg} = \\frac{-1}{m}\\sum_{i=1}^{m}\\left(Y\\log(\\hat{Y}) + (1-Y)\\log(1-\\hat{Y})\\right) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\left\\|W^{[l]}  \\right\\|^{2}_{F}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Where : \n",
        "- $W^{[l]}$ is the weight matrice of the layer $l$\n",
        "- $\\left\\|.\\right\\|_{F}$ is the Frobenius norm\n",
        "- $\\lambda$ is a regularization hyperparameter $\\geq$ 0\n",
        "\n",
        "So, as $\\lambda$ increase, the regularization term is shrinking the weights toward zero. The network is getting simpler and so, less prone to overfitting (in fact, it tends to the logistic regression which has high bias). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9daa964",
      "metadata": {
        "id": "c9daa964"
      },
      "outputs": [],
      "source": [
        "def compute_cost_L2regularization(yhat:np.ndarray, Y:np.ndarray, layers_dim:list, parameters:dict, lambd:float, mini_batch_size:int=None):\n",
        "    \"\"\"\n",
        "    Compute the cost function, sum of the loss function, with L2 regularization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    yhat : np.array of shape (1, m)\n",
        "        Probability vector corresponding to the label predictions. It's actually\n",
        "        the activation at the layer L : AL.\n",
        "    Y : np.array of shape (1, m)\n",
        "        True \"label\" vector.\n",
        "    layers_dims : list of length (number of layers + 1)\n",
        "        Contain the input size and each layer size.\n",
        "    parameters : dict\n",
        "        Output of initialize_parameters_deep().\n",
        "    lambd : float\n",
        "        Regularization factor\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cost : float\n",
        "        Cross-entropy cost.\n",
        "\n",
        "    \"\"\"\n",
        "    AL = yhat\n",
        "    m = AL.shape[1]\n",
        "    cross_entropy_cost = compute_cost(AL, Y, mini_batch_size)\n",
        "    \n",
        "    somme = 0\n",
        "    for l in range(1, len(layers_dim)):\n",
        "        W = parameters[\"W\"+str(l)]\n",
        "        somme = somme + np.sum(np.square(W))\n",
        "    \n",
        "    L2_regularization_cost = (1/m)*(lambd/2) * somme\n",
        "    cost = cross_entropy_cost + L2_regularization_cost\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19b13a6",
      "metadata": {
        "id": "f19b13a6"
      },
      "source": [
        "#### [5.1.2 - Dropout Regularization](#table) <a class=\"anchor\" id=\"dropout\"></a>\n",
        "Dropout is an other regularization technique that could be implement like the L2 regularization. Rather than working on the weight matrices, it takes effect directly on a node of a layer $l$ : it goes through the network and eliminates a node with a certain probability. Using *Dropout* leads to a diminished network for each $m$ example and then, it ends up by training a much smaller network (and therefore a simpler one) for each example. \n",
        "\n",
        "**Implementation :**\n",
        "- First, we set a dropout mask $d^{[l]}$ with the size of the actual activation layer $A^{[l]}$ such as the probability of **keeping** the node is defined by the hyperparameter $keep\\_prob$ :\n",
        "$$d^{[l]} = random([A^{[l]}]) < keep\\_prob$$\n",
        "\n",
        "- Then, we turn off some activation nodes, regarding the dropout mask $d$ :\n",
        "$$A^{[l]} = A^{[l]} \\times d^{[l]}$$ \n",
        "\n",
        "- Finally, even if the activation layer $A^{[l]}$ has been reduced, we still don't want to reduce the expected value of the pre-activation next layer $Z^{[l+1]}$. So, we must scaling up the reduced activation layer such as :\n",
        "$$A^{[l]} = A^{[l]} / keep\\_prob$$ \n",
        "\n",
        "This technique is actually called **Inverted Dropout Technique**.\n",
        "\n",
        "<br>\n",
        "\n",
        "*Note :*\n",
        "- *In the [backward part](#going_bwd), each dropout mask for each layer is stored in the `listD` parameter*\n",
        "- *It's common to use dropout for layers with a lot of parameters (in order to prevent potential overfitting). At least, it is common to use a large `keep_prob` for layers who should not have lot of parameters*\n",
        "- *We won't use dropout at test (validation) time because we don't want the output to be random. This would just add noise to the predicitons*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JNV2TNyDP5iO",
      "metadata": {
        "id": "JNV2TNyDP5iO"
      },
      "source": [
        "### [5.2 - Optimizations](#table) <a class=\"anchor\" id=\"optim\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab3893d",
      "metadata": {
        "id": "0ab3893d"
      },
      "source": [
        "#### [5.2.1 - Mini-Batch Gradient Descent](#table) <a class=\"anchor\" id=\"minibatch\"></a>\n",
        "As defined earlier (see [part 1.2](#section_1_2)) the mini-batch technique is a way to speeding-up the learning of the network. Indeed, usually the network processes the *entire* training set before one step of gradient descent. With mini-batches, gradient descent makes progress *before* finishing of processing the entire training set. The network must contain an extra inner loop to go through each mini-batch (see [part 4.1](#training_fct)). Also, a mini-batch cost function is computed to keep track of the variation of the loss into the mini-batch computing.\n",
        "\n",
        "Mini-batch Gradient Descent makes a param update after seeing just a subset of examples so, the direction of the update has some variance and the path taken will \"oscillate\" toward convergence.\n",
        "\n",
        "For some problems with large training set, the **Stochastic Gradient Descent** could be used by setting the mini-batch size to 1. It calculates the error and updates the model for each example in the training dataset. The increased model update frequency could result in faster learning and the noisy update can allow the model to avoid local minima. The main disavantages is losing the benefit of vectorisation (since it processes a single example at a time) and the cost doesn't converge to the real minimum because of the noise introduced by taking care of one single example at a time.\n",
        "\n",
        "<br>\n",
        "\n",
        "*Note :* \n",
        "- *The basic Batch Gradient Descent is usually chosen for small training set (m $\\leq$ 2000 examples)*\n",
        "- *The mini-batch optimization technique provides a fair improvement of time computing if the number of training examples is over 2000*\n",
        "- *The mini-batch size is set as $2^{2n}$ (64, 128, 512...) to mirroring the way of how the computer memory works*\n",
        "- *We must be sure that each mini-batch fits in the CPU/GPU memory*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4f66ca",
      "metadata": {
        "id": "dd4f66ca"
      },
      "source": [
        "#### [5.2.2 - Adding Momentum](#table) <a class=\"anchor\" id=\"momentum\"></a>\n",
        "\n",
        "Adding momentum to the gradient descent optimization algorithm allows the search to overcome the oscillations of noisy gradients. In particular, it tends to average one direction of the oscillations to accelerate the convergence (helps to learn in strait line). Momentum can be associated as a velocity since it controls the gradients $dW$ and $db$ which are the variations of $W$ and $b$. \n",
        "\n",
        "Gradient Descent with momentum uses *exponentially weighted moving averages* (EWMA) statistic notions. The EWMA could be define like : \n",
        "\n",
        "$$ v_{\\theta} = \\beta v_\\theta + (1-\\beta) \\theta_t $$\n",
        "\n",
        "Where :\n",
        "- $v_\\theta$ is the average value of the parameter $\\theta$ also called the \"velocity\" of the momentum optimization\n",
        "- $\\beta$ is the momentum parameter : it represents the number of 'step' taking into account for computing the average $v_\\theta$ defined by the formula : $\\frac{1}{1-\\beta}$\n",
        "- $t$ si the EWMA \"step\"\n",
        "- $\\theta_t$ is the \"t\" measured parameter \n",
        "\n",
        "Usually, we also do a \"bias correction\" which makes the computations of the EWMA more accurate during the inital phases : \n",
        "\n",
        "$$v_t >>> \\frac{v_t}{1-\\beta^t}$$\n",
        "\n",
        "So, for each epoch, we have to compute $v_{dw}$ and $v_{db}$ the average parameters and we must redefine the gradient descent update parameter step :\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "        v_{dw} = \\beta v_{dw} + (1-\\beta)dW\\\\\n",
        "        v_{db} = \\beta v_{db} + (1-\\beta)db\n",
        "    \\end{cases}\\,\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "        W = W - \\alpha v_{dw}\\\\\n",
        "        b = b - \\alpha v_{db}\\\\\n",
        "    \\end{cases}\\,\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "A common value for momentum $\\beta$ is 0.9 such as the EWMA uses arround $\\frac{1}{1-0.9} = 10$ gradients to compute the averages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1b7892",
      "metadata": {
        "id": "9b1b7892"
      },
      "outputs": [],
      "source": [
        "def initialize_velocity(parameters:dict) -> dict:\n",
        "    \"\"\"\n",
        "    Initializes the velocity for the momentum optimization as a python \n",
        "    dictionary with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the \n",
        "                corresponding gradients/parameters.\n",
        "    Parameters\n",
        "    ----------\n",
        "    parameters : dict\n",
        "        Contain the weights and bias at the layer l\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    v : dict\n",
        "        Contain the current velocity at the layer l for the corresponding gradients.\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2\n",
        "    v = {}\n",
        "    \n",
        "    # Initialize velocity\n",
        "    for l in range(1, L + 1):\n",
        "        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
        "        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)        \n",
        "    return v\n",
        "\n",
        "\n",
        "\n",
        "def momentum_update_parameters(parameters:dict, grads:dict, v:dict, beta:float, learning_rate:float):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parameters : dict\n",
        "        Containing the parameters.\n",
        "    grads : dict\n",
        "        Contain the grads output of the backward pass.\n",
        "    v : dict\n",
        "        Contain the current velocity at the layer l for the corresponding gradients.\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    beta : float\n",
        "        Momentum hyperparameter\n",
        "    learning_rate : float\n",
        "        Tune the rate of learning.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    parameters : dict\n",
        "        Contain the update parameters.\n",
        "    v : dict\n",
        "        Contain the current velocity at the layer l for the corresponding gradients.\n",
        "    \n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2\n",
        "    \n",
        "    # Momentum update for each parameter\n",
        "    for l in range(1, L+1):\n",
        "        v[\"dW\" + str(l)] = beta * v[\"dW\" + str(l)] + (1 - beta) * grads[\"dW\" + str(l)]\n",
        "        v[\"db\" + str(l)] = beta * v[\"db\" + str(l)] + (1 - beta) * grads[\"db\" + str(l)] \n",
        "        \n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - \\\n",
        "            learning_rate * v[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - \\\n",
        "            learning_rate * v[\"dW\" + str(l)]\n",
        "    \n",
        "    return parameters, v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15485a39",
      "metadata": {
        "id": "15485a39"
      },
      "source": [
        "#### [5.2.3 - Adam optimization](#table) <a class=\"anchor\" id=\"adam\"></a>\n",
        "\n",
        "**Ada**ptative **M**oment Estimation or Adam optimization technique is a combination of two optimization techniques : Momentum and RMSprop. \n",
        "RMSprop can be define like Momentum optimization : it modifies the update parameter's step with the help of the exponential weighted moving average :\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "        S_{dW} = \\beta + (1-\\beta)dW^2\\\\\n",
        "        S_{db} = \\beta + (1-\\beta)db^2\n",
        "    \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "        W = W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}+\\epsilon}\\\\\n",
        "        b = b - \\alpha \\frac{db}{\\sqrt{S_{db}}+\\epsilon}\n",
        "    \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Finally, Adam optimization combines Momentum and RMSprop to updates parameters $W$ and $b$ regarding the hyperparameters $\\beta_1$ and $\\beta_2$ and the Momentum and RMSprop parameters $v_{dW}, v_{db}, S_{dW}, S_{db}$ with a EWMA bias correction so :\n",
        "\n",
        "\\begin{equation}\n",
        "   \\begin{cases}\n",
        "   v_{dW} = \\frac{v_{dW}}{1-\\beta^t}, v_{db} = \\frac{v_{db}}{1-\\beta^t} \\\\\n",
        "   S_{dW} = \\frac{S_{dW}}{1-\\beta^t}, S_{db} = \\frac{S_{db}}{1-\\beta^t}\n",
        "   \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Leads to :\n",
        "\n",
        "\\begin{equation}\n",
        "   \\begin{cases}\n",
        "       W = W - \\alpha \\frac{v_{dW}}{\\sqrt{S_{dW}}+\\epsilon} \\\\\n",
        "       b = b - \\alpha \\frac{v_{db}}{\\sqrt{S_{db}}+\\epsilon}\n",
        "   \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Where : \n",
        "- $\\beta_1$ and $\\beta_2$ are Adam's hyperparameters\n",
        "- $\\beta_1, \\beta_2, \\epsilon$ are usually set respectively to 0.9, 0.999 and 10e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd2caeb",
      "metadata": {
        "id": "abd2caeb"
      },
      "outputs": [],
      "source": [
        "def initialize_adam(parameters:dict):\n",
        "    \"\"\"\n",
        "    Initializes Adam parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parameters : dict\n",
        "         Initializes v and s as two python dictionaries.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    v : dict\n",
        "        Will contain the exponentially weighted average of the gradient. Initialized with zeros.\n",
        "    s : dict\n",
        "        Will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n",
        "\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    for l in range(1, L + 1):\n",
        "        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
        "        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
        "        s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
        "        s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
        "        \n",
        "    return v, s\n",
        "\n",
        "\n",
        "\n",
        "def adam_update_parameters(parameters:dict, grads:dict, v:dict, s:dict, t:int, \n",
        "                           learning_rate:float=0.01, beta1:float=0.9, beta2:float=0.999, epsilon:float=1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    parameters : dict\n",
        "        Contains the parameters for the current layer l\n",
        "    grads : dict\n",
        "        Contains the grads output of the backward pass.\n",
        "    v : dict\n",
        "        Contains the exponentially weighted average of the gradient.\n",
        "    s : dict\n",
        "        Contains the exponentially weighted average of the squared gradient.\n",
        "    t : int\n",
        "        Adam variable, counts the number of taken steps\n",
        "    learning_rate : float\n",
        "        Learning_rate\n",
        "    beta1 : float\n",
        "        Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 : float\n",
        "        Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon : float\n",
        "        Hyperparameter preventing division by zero in Adam updates\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    parameters : dict\n",
        "        Containing the parameters for the current layer l\n",
        "    v : dict\n",
        "        Will contain the exponentially weighted average of the gradient. Initialized with zeros.\n",
        "    s : dict\n",
        "        Will contain the exponentially weighted average of the squared gradient. Initialized with zeros.\n",
        "    v_corrected : dict\n",
        "        Blabla\n",
        "    s_corrected : dict\n",
        "        Blabla\n",
        "\n",
        "    \"\"\" \n",
        "    L = len(parameters) // 2\n",
        "    v_corrected = {}                         \n",
        "    s_corrected = {}\n",
        "    \n",
        "    for l in range(1, L+1):\n",
        "        # Exponentially moving average of the gradients\n",
        "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1 - beta1) * grads[\"dW\" + str(l)]\n",
        "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1 - beta1) * grads[\"db\" + str(l)]\n",
        "        \n",
        "        # Compute bias-corrected for first moment estimate\n",
        "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - pow(beta1, t))\n",
        "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - pow(beta1, t))\n",
        "        \n",
        "        # Exponentially moving average of the squared gradients\n",
        "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1 - beta2) * pow(grads[\"dW\" + str(l)], 2)\n",
        "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1 - beta2) * pow(grads[\"db\" + str(l)], 2)\n",
        "        \n",
        "        # Compute bias-corrected for second moment estimate\n",
        "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - pow(beta2, t))\n",
        "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - pow(beta2, t))\n",
        "        \n",
        "        # Update parameters\n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)\n",
        "        \n",
        "        return parameters, v, s, v_corrected, s_corrected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0194b440",
      "metadata": {
        "id": "0194b440"
      },
      "source": [
        "Let's now, build our new model function `NN_model_2` based on the previous model function `NN_model`, with respect to the regularization and optimization hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0743d7",
      "metadata": {
        "id": "ec0743d7"
      },
      "outputs": [],
      "source": [
        "def NN_model_2(X:np.ndarray, Y:np.ndarray, layers_dims:list, optimizer:str='gd', learning_rate:float=0.0075, \n",
        "               mini_batch_size:int=64, lambd:float=0., keep_prob:float=1., beta:float=0.9, beta1:float=0.9, \n",
        "               beta2:float=0.999, epsilon:float=1e-8, epochs:int=1000):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    With potential regularization techniques and optimization techniques.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.array\n",
        "        Input features.\n",
        "    Y : np.array of shape (1, m)\n",
        "        True \"label\" vector (containing 0 if Pikachu, 1 if Rondoudou)\n",
        "    layers_dims : list of length (number of layers + 1)\n",
        "        Contain the input size and each layer size.\n",
        "    optimizer : str, optional\n",
        "        Optimization technique to use. The default is gradient descent 'gd'.\n",
        "    learning_rate : float, optional\n",
        "        Learning rate of the gradient descent update rule. The default is 0.0075.\n",
        "    mini_batch_size : int, optional\n",
        "        Size of the mini_batch.\n",
        "    lambd : float, optional\n",
        "        L2 regularization parameter. The default is 0.\n",
        "    keep_prob : float, optional\n",
        "        Dropout regularization parameter. The default is 1.\n",
        "    beta : float, optional\n",
        "        Momentum optimization parameter. The default is 0.9.\n",
        "    beta1 : float, optional\n",
        "        Adam optimization parameter. The default is 0.9.\n",
        "    beta2 : float, optional\n",
        "        Adam optimization parameter. The default is 0.999.\n",
        "    epsilon : float, optional\n",
        "        Adam optimization parameter. The default is 1e-8.\n",
        "    epochs : int, optional\n",
        "         Number of iterations of the optimization loop. The default is 1000.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    parameters : dict\n",
        "        Parameters learnt by the model.\n",
        "    costs : list\n",
        "        List of costs for each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    costs = []\n",
        "    t = 1\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "    \n",
        "    # Initialize parameters optimizer\n",
        "    if optimizer == \"gd\":\n",
        "        pass\n",
        "    elif optimizer == \"momentum\":\n",
        "        v = initialize_velocity(parameters)\n",
        "    elif optimizer == \"adam\":\n",
        "        v, s = initialize_adam(parameters)\n",
        "    \n",
        "    print(f\"Training is starting with minibatch size = {mini_batch_size} ...\")\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Chose a minibatch per epoch\n",
        "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
        "        cost_total = 0\n",
        "        \n",
        "        # Training through minibatch\n",
        "        for minibatch in minibatches:\n",
        "            # Select a minibatch\n",
        "            (minibatch_X, minibatch_Y) = minibatch \n",
        "        \n",
        "            # Forward pass\n",
        "            yhat, caches, listD = forward(minibatch_X, parameters, keep_prob)\n",
        "            \n",
        "            # Compute cost and add to the cost total\n",
        "            if lambd == 0:\n",
        "                cost = compute_cost(yhat, minibatch_Y, mini_batch_size)\n",
        "            else:\n",
        "                cost = compute_cost_L2regularization(yhat ,minibatch_Y, layers_dims, parameters, lambd, mini_batch_size)\n",
        "            cost_total += cost\n",
        "        \n",
        "            # Backward pass\n",
        "            grads = backward(yhat, minibatch_Y, caches, lambd, listD, keep_prob)\n",
        "        \n",
        "            # Update parameters\n",
        "            if optimizer == \"gd\":\n",
        "                parameters = update_parameters(parameters, grads, learning_rate)\n",
        "            elif optimizer == \"momentum\":\n",
        "                parameters, v = momentum_update_parameters(parameters, grads, v, beta, learning_rate)\n",
        "            elif optimizer == \"adam\":\n",
        "                parameters, v, s, _, _ = adam_update_parameters(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
        "                t += 1\n",
        "        # Compute average cost\n",
        "        cost_avg = cost_total / m\n",
        "            \n",
        "        # Printing\n",
        "        if epoch==1 or epoch % 100 == 0 or epoch == epochs: #- 1:\n",
        "            print(\"{} ------- Cost after iteration {}: {:.4f}\".format(\n",
        "                datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S'), \n",
        "                epoch, \n",
        "                cost_avg))\n",
        "        if epoch % 100 == 0 or epoch == epochs:\n",
        "            costs.append(cost_avg)\n",
        "            \n",
        "    return parameters, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64231ac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64231ac7",
        "outputId": "80b7395c-2748-4945-f24d-067c38c73246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length pikachu dataset : 98\n",
            "Length Rondoudou dataset : 76\n",
            "\n",
            "----------------------\n",
            "\n",
            "data_train's shape: (30000, 140)\n",
            "data_val's shape: (30000, 34)\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing\n",
        "data_train_orig, label_train, data_val_orig, label_val, classes = load_data()\n",
        "\n",
        "m_train_2 = data_train_orig.shape[0]\n",
        "m_test_2 = data_val_orig.shape[0]\n",
        "\n",
        "# Reshape the training and test examples \n",
        "data_train_flatten_2 = data_train_orig.reshape(m_train_2, -1).T\n",
        "data_val_flatten_2 = data_val_orig.reshape(m_test_2, -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "data_train_2 = data_train_flatten_2/255.\n",
        "data_val_2 = data_val_flatten_2/255.\n",
        "\n",
        "label_train_2 = label_train.T\n",
        "label_val_2 = label_val.T\n",
        "\n",
        "print (\"\\n----------------------\\n\")\n",
        "print (\"data_train's shape: \" + str(data_train_2.shape))\n",
        "print (\"data_val's shape: \" + str(data_val_2.shape))\n",
        "print (\"\\n----------------------\\n\")\n",
        "\n",
        "n_x_2 = data_train_orig.shape[1]*data_train_orig.shape[2]*data_train_orig.shape[3]\n",
        "n_y_2 = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734007c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "734007c0",
        "outputId": "b89ad331-5339-4d2b-b3e6-0e23e92c26bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training is starting with minibatch size = 64 ...\n",
            "2022-04-21_12.16.23 ------- Cost after iteration 1: 0.6721880481245561\n",
            "2022-04-21_12.16.37 ------- Cost after iteration 100: 0.08118614168403186\n",
            "2022-04-21_12.16.50 ------- Cost after iteration 200: 0.004229460522253018\n",
            "2022-04-21_12.17.03 ------- Cost after iteration 300: 0.002257582105592734\n",
            "2022-04-21_12.17.16 ------- Cost after iteration 400: 0.0013789982106878148\n",
            "2022-04-21_12.17.29 ------- Cost after iteration 500: 0.0009743559452495649\n",
            "2022-04-21_12.17.42 ------- Cost after iteration 600: 0.0007240775113913614\n",
            "2022-04-21_12.17.55 ------- Cost after iteration 700: 0.0005558182328832589\n",
            "2022-04-21_12.18.08 ------- Cost after iteration 800: 0.0004445863091180383\n",
            "2022-04-21_12.18.21 ------- Cost after iteration 900: 0.0003515737431901355\n",
            "2022-04-21_12.18.34 ------- Cost after iteration 1000: 0.00028292321242051475\n",
            "Training Accuracy: 1.0\n",
            "Validation Accuracy: 0.9411764705882353\n"
          ]
        }
      ],
      "source": [
        "# Training a 4-layers model with Adam optimization\n",
        "layers_dims_2 = [n_x_2, 20, 10, 5, n_y_2]\n",
        "parameters, costs = NN_model_2(data_train_2,\n",
        "                               label_train_2,\n",
        "                               layers_dims_2,\n",
        "                               optimizer = 'adam',\n",
        "                               learning_rate = 0.001,\n",
        "                               epochs = 1000)\n",
        "\n",
        "pred_train = predict(data_train_2, label_train_2, parameters)\n",
        "pred_test = predict(data_val_2, label_val_2, parameters, training_set=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08523b7",
      "metadata": {
        "id": "c08523b7"
      },
      "source": [
        "\n",
        "## [6 - Conclusion](#table) <a class=\"anchor\" id=\"conclusion\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d6d98d",
      "metadata": {
        "id": "51d6d98d"
      },
      "source": [
        "## [5.2.2 - Batch Normalization](#table) <a class=\"anchor\" id=\"BN\"></a>\n",
        "\n",
        "A common way to speed-up the learning is to normalized mini-batches such as mean and variance of the pre-activation layer is respectively 0 and 1. It permits to keep the features at the same range and prevents covariate shift. Finally, the layer parameters are more independent.\n",
        "\n",
        "Usually, it applies on the pre-activation layer $Z^{[l-1]}$ to train the next layer parameters faster :\n",
        "$$ Z_{norm} = \\frac{Z-\\mu}{\\sqrt{\\sigma^{2} + \\epsilon}}$$ \n",
        "Such as $\\mu^{[l]}(z_{norm}^{(i)}) = 0$ and $\\sigma^{[l]}(z_{norm}^{(i)}) = 1$\n",
        "\n",
        "\n",
        "Where :\n",
        "- $\\mu = \\frac{1}{m} \\sum^{}_{i}z^{(i)}$ is the mean regarding each node $i$\n",
        "- $\\sigma^2 = \\frac{1}{m}\\sum^{}_{i}(z^{(i)} - \\mu^{(i)})^2$ is the standard deviation regarding each node $i$\n",
        "- $\\epsilon$ a parameter sets to prevent zero division\n",
        "\n",
        "Since we want to have control on the mean and the variance distributions of $Z_{norm}$ we define :\n",
        "\n",
        "$$ \\tilde{Z} = \\beta_{1} Z_{norm} + \\beta_{2} Z_{norm} $$\n",
        "\n",
        "*Note : covariate shift occurs when the distribution of independent (input) variables can change over time while the labeling stays the same (ex : training on real dogs and then trying to detect cartoon dogs).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SWrdIIonbntC",
      "metadata": {
        "id": "SWrdIIonbntC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "binary_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}