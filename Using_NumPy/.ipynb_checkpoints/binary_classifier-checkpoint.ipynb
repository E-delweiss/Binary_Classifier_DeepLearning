{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9280e740-0300-44fe-8ca9-09c4b4f7c7c7",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "* [Data](#chapter1)\n",
    "    * [Data loading](#section_1_1)\n",
    "    * [Handling mini-batches](#section_1_2)\n",
    "* [Pieces of a Neural Network](#chapter2)\n",
    "    * [The Forward Propagation](#fwd_prop)\n",
    "    * [The Cost function](#cost_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64db3b2-f832-4f17-8df6-2143bc32d5dc",
   "metadata": {},
   "source": [
    "# NumPy Binary Classifier \n",
    "\n",
    "In this notebook, I would like to show you what is the main logic behind the common deep learning frameworks. I found this interesting since I usually tweet the hyperparameters framework's functions without really knowing what is changing into the equations.\n",
    "\n",
    "So, a matrix point of view gives a better overview of *how a Neural Network works*, and once you're familiar with those notions, you will be able to construct deeper and more complex N.N. with the help of deep learning frameworks, knowing what you are doing. Also, it will help you to tune your model, once it has been trained, to improve it.\n",
    "\n",
    "The goal of this classifier is to detect whether an image is a Pikachu or a Jigglypuff (english version of Rondoudou)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da41b93-c61a-4104-8616-3da572eb0ee9",
   "metadata": {},
   "source": [
    "## Data <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "First we need to work on the dataset to feed the neural network with the right dimensions, types etc. For the purpuse of the exercice, I didn't chose a large dataset, it contains :\n",
    "- 98 Pikachu images (label 0)\n",
    "- 76 Jigglypuff [Rondoudou] images (label 1)\n",
    "\n",
    "So 174 images that we have to randomize and split into train and test sets. \n",
    "\n",
    "### Data loading <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "We are going to create a function which returns train/val sets and their correspondant labels. This function should let us decide the size of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8beb104-9992-484c-be1a-01a3b24551aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da9f4ea-64f8-4eda-9f3d-72f7710448e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(val_size=0.2):\n",
    "    \"\"\"\n",
    "    Converts images to arrays and returns them randomized through training and \n",
    "    validation set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    val_size : float, optional\n",
    "        Part of validation set. The default is 0.2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train : np.array of shape (nb_img, HEIGHT, WIDTH, nb_chans)\n",
    "        Training set.\n",
    "    label_train : np.array of shape (nb_img, 1)\n",
    "        Labels of the training set.\n",
    "    data_val : np.array of shape (nb_img, HEIGHT, WIDTH, nb_chans)\n",
    "        Validation set.\n",
    "    label_val : np.array of shape (m, 1)\n",
    "        Labels of the validation set.\n",
    "    classes : np.array of shape (2,)\n",
    "        Classe names : Pikachu / Rondoudou. They are encode in bytes.\n",
    "\n",
    "    \"\"\"\n",
    "    list_pikachu = glob.glob('../data/pikachu/*')\n",
    "    list_rondoudou = glob.glob('../data/rondoudou/*')\n",
    "    \n",
    "    HEIGHT = 100\n",
    "    WIDTH = 100\n",
    "    CHANNEL = 3\n",
    "    \n",
    "    classes = np.array([b'Pikachu', b'Rondoudou'])\n",
    "    \n",
    "    # Initialisations\n",
    "    size_dataset = len(list_pikachu) + len(list_rondoudou)\n",
    "    dataset_arr = np.zeros((size_dataset, HEIGHT, WIDTH, CHANNEL))\n",
    "    label = np.zeros((size_dataset, 1), dtype='int')\n",
    "    \n",
    "    # Generating a Pikachu array-type dataset\n",
    "    for k in range(len(list_pikachu)):\n",
    "        with Image.open(list_pikachu[k]) as im :\n",
    "            im = im.resize((HEIGHT, WIDTH), resample=Image.BICUBIC)\n",
    "            im = im.convert(\"RGB\")\n",
    "        img_arr = np.array(im)\n",
    "        dataset_arr[k] = img_arr\n",
    "        \n",
    "    # Generating a Rondoudou array type dataset\n",
    "    i=0\n",
    "    for k in range(len(list_pikachu), len(dataset_arr)):\n",
    "        with Image.open(list_rondoudou[i]) as im2 :\n",
    "            im2 = im2.resize((HEIGHT, WIDTH), resample=Image.BICUBIC)\n",
    "            im2 = im2.convert(\"RGB\")\n",
    "        img_arr = np.array(im2)\n",
    "        dataset_arr[k] = img_arr\n",
    "        label[k] = 1\n",
    "        i+=1\n",
    "    \n",
    "    # Randomizing\n",
    "    n_samples = dataset_arr.shape[0]\n",
    "    n_val = int(val_size * n_samples)\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "    train_indices = shuffled_indices[:-n_val] \n",
    "    val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "    data_train = dataset_arr[train_indices]\n",
    "    label_train = label[train_indices]\n",
    "    \n",
    "    data_val = dataset_arr[val_indices]\n",
    "    label_val = label[val_indices]\n",
    "    \n",
    "    return data_train, label_train, data_val, label_val, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d5d12-d641-49e2-9994-ee7872d1bb7c",
   "metadata": {},
   "source": [
    "### Handling mini-batches <a class=\"anchor\" id=\"section_1_2\"></a>\n",
    "Using mini-batches is an optimization technique and permits to let gradient descent makes progress *before* finishing of precessing the *entire* training set. So, we need to create a function capable of splitting a training dataset into mini-batches of size `mini_batch_size` with the corresponding labels for each image in each mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cbde7f-06cc-4dfc-a694-aef6295db09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * inc : (k+1) * inc]\n",
    "        mini_batch_Y = shuffled_Y[:, k * inc : (k+1) * inc]\n",
    "    \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, (k+1) * inc : ]\n",
    "        mini_batch_Y = shuffled_Y[:, (k+1) * inc : ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b8445-e79e-48fa-a08f-773ad4c11a2c",
   "metadata": {},
   "source": [
    "## Pieces of a Neural Network <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "In this section, we'll develop functions that are usefull to construct a neural network of size `L` with any number of units for each layer.\n",
    "\n",
    "### The Forward Propagation <a class=\"anchor\" id=\"fwd_prop\"></a>\n",
    "A neural network adjusts each hidden-layer units weights dependantly to the previous layer and go from the **input layer** to the last layer called the **output layer**. This step is called the **forward pass**. Each layer has those following variables of interest :\n",
    "- $W^{[l]}$ the weights of the $l^{th}$ layer\n",
    "- $b^{[l]}$ the bias of the $l^{[th]}$ layer\n",
    "- $Z^{[l]}$ the pre-activation value of the $l^{[th]}$ layer : it results of a linear calculus and depends of $W^{[l]}$, $b^{[l]}$, and $A^{[l-1]}$. Note that $A^{[l-1]}$ is actually the features $X$ for the first layer. \n",
    "- $A^{[l]}$ the activation value of the $l^{[th]}$ layer computes from a given activation function.\n",
    "\n",
    "\n",
    "At layer `l`, each neuron is composed of two parts : a linear part which gives `z`, and an activation part to \"activate\" the neurone, which gives `a`. Using matrix operations to reprensent all the units for a given layer, we got the equations below :\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      Z^{[l]} = W^{[l]}A^{[l-1]} + b\\\\\n",
    "      A^{[l]} = g(Z^{[l]})\n",
    "    \\end{cases}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Note :\n",
    "- We use lower cases to talk about a neuron (or a unit) and capital cases to represent a layer of neurons.\n",
    "- `g` represents a given activation function for this neuron.\n",
    "\n",
    "\n",
    "### Initialization\n",
    "The first layer's weights and bias are initialized randomly to \"initiate the learning\". Here we'll use the Hô technique to initia...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e25a1d-920d-465f-b86b-93d72b903187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z, activation):\n",
    "    \"\"\"\n",
    "    Calculate the activation function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : np.array\n",
    "        pre-activation parameter.\n",
    "    activation : str\n",
    "        Name of the activation function\n",
    "    Returns\n",
    "    -------\n",
    "    A : np.array of shape Z.shape\n",
    "        Post-activation parameter\n",
    "\n",
    "    \"\"\"\n",
    "    if activation == 'sigmoid':\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "    elif activation == 'relu':\n",
    "        A = np.maximum(0,Z) \n",
    "    return A\n",
    "\n",
    "\n",
    "def forward_pass(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward pass\n",
    "    LINEAR->ACTIVATION layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np.array of shape (n[l-1], m)\n",
    "        Activations from previous layer (or input data).\n",
    "    W : np.array of shape (n[l], n[l-1])\n",
    "        Weights matrix.\n",
    "    b : np.array of size (n[l], 1)\n",
    "        Bias vector.\n",
    "    activation : str\n",
    "        Name of activation function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : np.array\n",
    "        Post-activation value.\n",
    "    cache : tuple\n",
    "        containing \"linear_cache\" and \"activation_cache\". Storing variable for \n",
    "        computing the backward pass efficiently.\n",
    "\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation_function(Z, activation)\n",
    "\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3046a",
   "metadata": {},
   "source": [
    "### The Cost function<a class=\"anchor\" id=\"cost_fct\"></a>\n",
    "The cost function computes how much the predicting result (at the output layer) is far from a given ground truth. The aim of a neural network is to maximise the cost function by tweeting the weights and ..\n",
    "\n",
    "\\begin{equation}\n",
    "    cost = \\frac{-1}{m}\\sum_{i=1}^{m}\\left(Y\\log(\\hat{Y}) + (1-Y)\\log(1-\\hat{Y})\\right) \\\\\n",
    "    {cost}_{L2reg} = \\frac{-1}{m}\\sum_{i=1}^{m}\\left(Y\\log(\\hat{Y}) + (1-Y)\\log(1-\\hat{Y})\\right) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\left\\|W^{[l]}  \\right\\|_{F}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a582261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(yhat, Y, mini_batch_size=None):\n",
    "    \"\"\"\n",
    "    Compute the cost function, sum of the loss function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yhat : np.array of shape (1, m)\n",
    "        Probability vector corresponding to the label predictions. It's actually\n",
    "        the activation at the layer L : AL.\n",
    "    Y : np.array of shape (1, m)\n",
    "        True \"label\" vector.\n",
    "    mini_batch_size : int, optionnal\n",
    "        Arg used to trigger the normalization of the cost by 1/m if there is no\n",
    "        mini_batches. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        Cross-entropy cost function with or without dividing by number of \n",
    "        training examples\n",
    "\n",
    "    \"\"\"\n",
    "    AL = yhat\n",
    "    cost = np.sum(-np.log(AL)*Y - np.log(1-AL)*(1-Y))\n",
    "    \n",
    "    if mini_batch_size is None:\n",
    "        m = Y.shape[1]\n",
    "        cost = (1/m) * cost\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def compute_cost_L2regularization(yhat, Y, layers_dim, parameters, lambd, mini_batch_size=None):\n",
    "    \"\"\"\n",
    "    Compute the cost function, sum of the loss function, with L2 regularization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yhat : np.array of shape (1, m)\n",
    "        Probability vector corresponding to the label predictions. It's actually\n",
    "        the activation at the layer L : AL.\n",
    "    Y : np.array of shape (1, m)\n",
    "        True \"label\" vector.\n",
    "    parameters : dict\n",
    "        Output of initialize_parameters_deep().\n",
    "    lambd : float\n",
    "        Regularization factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        Cross-entropy cost.\n",
    "\n",
    "    \"\"\"\n",
    "    AL = yhat\n",
    "    m = AL.shape[1]\n",
    "    cross_entropy_cost = compute_cost(AL, Y, mini_batch_size)\n",
    "    \n",
    "    somme = 0\n",
    "    for l in range(1, len(layers_dim)):\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        somme = somme + np.sum(np.square(W))\n",
    "    \n",
    "    L2_regularization_cost = (1/m)*(lambd/2) * somme\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
