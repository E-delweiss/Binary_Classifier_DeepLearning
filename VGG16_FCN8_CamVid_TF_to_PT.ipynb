{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16-FCN8-CamVid_TF_to_PT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9O7hG4qBLSECveMSjx9hA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Binary_Classifier_DeepLearning/blob/main/VGG16_FCN8_CamVid_TF_to_PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vbyflefu45i",
        "outputId": "9d250059-8145-48f1-dd11-3227d917c098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install torch-summary\n",
        "from torchsummary import summary\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset (zipped file)\n",
        "!gdown --id 0B0d9ZiqAgFkiOHR1NTJhWVJMNEU -O /tmp/fcnn-dataset.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKq36e_gvZXE",
        "outputId": "f2a90dc7-7dc3-446c-c859-b0f7feacc326"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0B0d9ZiqAgFkiOHR1NTJhWVJMNEU\n",
            "To: /tmp/fcnn-dataset.zip\n",
            "100% 126M/126M [00:01<00:00, 120MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the downloaded dataset to a local directory: /tmp/fcnn\n",
        "local_zip = '/tmp/fcnn-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/fcnn')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "xzJQwHUIvbaR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pixel labels in the video frames\n"
      ],
      "metadata": {
        "id": "0yMwbAj4v5eV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO\n",
        "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
        "  '''\n",
        "  Preprocesses the dataset by:\n",
        "    * resizing the input image and label maps\n",
        "    * normalizing the input image pixels\n",
        "    * reshaping the label maps from (height, width, 1) to (height, width, 12)\n",
        "\n",
        "  Args:\n",
        "    t_filename (string) -- path to the raw input image\n",
        "    a_filename (string) -- path to the raw annotation (label map) file\n",
        "    height (int) -- height in pixels to resize to\n",
        "    width (int) -- width in pixels to resize to\n",
        "\n",
        "  Returns:\n",
        "    image (tensor) -- preprocessed image\n",
        "    annotation (tensor) -- preprocessed annotation\n",
        "  '''\n",
        "\n",
        "  # Convert image and mask files to tensors \n",
        "  img_raw = PIL.Image.open(t_filename)\n",
        "  img_raw = torchvision.transforms.ToTensor()(img_raw)\n",
        "  anno_raw = PIL.Image.open(a_filename)\n",
        "  anno_raw = torchvision.transforms.ToTensor()(anno_raw)\n",
        "  #img_raw = tf.io.read_file(t_filename)\n",
        "  #anno_raw = tf.io.read_file(a_filename)\n",
        "  #image = tf.image.decode_jpeg(img_raw)\n",
        "  #annotation = tf.image.decode_jpeg(anno_raw)\n",
        " \n",
        "  # Resize image and segmentation mask\n",
        "  image = tf.image.resize(image, (height, width,))\n",
        "  annotation = tf.image.resize(annotation, (height, width,))\n",
        "  image = tf.reshape(image, (height, width, 3,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  annotation = tf.reshape(annotation, (height, width, 1,))\n",
        "  stack_list = []\n",
        "\n",
        "  # Reshape segmentation masks\n",
        "  for c in range(len(class_names)):\n",
        "    mask = tf.equal(annotation[:,:,0], tf.constant(c))\n",
        "    stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
        "  \n",
        "  annotation = tf.stack(stack_list, axis=2)\n",
        "\n",
        "  # Normalize pixels in the input image\n",
        "  image = image/127.5\n",
        "  image -= 1\n",
        "\n",
        "  return image, annotation"
      ],
      "metadata": {
        "id": "YnfE1qdJwapC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show folders inside the dataset you downloaded\n",
        "!ls /tmp/fcnn/dataset1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOb1avJbyRkh",
        "outputId": "23bef772-c282-4ee9-be12-7f2479b3571f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations_prepped_test   images_prepped_test\n",
            "annotations_prepped_train  images_prepped_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PlIXDxeSzUsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CamVid_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, anno_map_dir, height=224, width=224):      \n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # generates the lists of image and label map paths\n",
        "        image_file_list = os.listdir(image_dir)\n",
        "        anno_map_file_list = os.listdir(anno_map_dir)\n",
        "        self.image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "        self.anno_map_paths = [os.path.join(anno_map_dir, fname) for fname in anno_map_file_list]\n",
        "\n",
        "        self.class_names = ['sky', 'building','column/pole', 'road', 'side walk', \n",
        "                            'vegetation', 'traffic light', 'fence', 'vehicle', \n",
        "                            'pedestrian', 'byciclist', 'void']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def preprocess(self, img_path, anno_path):\n",
        "        # reading\n",
        "        image = torchvision.io.read_image(img_path)\n",
        "        annotation = torchvision.io.read_image(anno_path)\n",
        "\n",
        "        # resizing\n",
        "        image = torchvision.transforms.Resize((self.height, self.width))(image)\n",
        "        annotation = torchvision.transforms.Resize((self.height, self.width))(annotation).to(torch.int32)\n",
        "\n",
        "        # Reshape segmentation masks\n",
        "        stack_list = []\n",
        "        for c in range(len(self.class_names)):\n",
        "            mask = torch.eq(b[0, :, :], torch.as_tensor(c, dtype=torch.int32))\n",
        "            stack_list.append(mask)\n",
        "        annotation = torch.stack(stack_list, dim=2)\n",
        "        \n",
        "        # Normalizing between -1 and 1\n",
        "        image = image / 127.5\n",
        "        image -= 1\n",
        "\n",
        "        return image, annotation\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.to_list()\n",
        "\n",
        "        image, annotation = self.preprocess(self.image_paths[idx], self.anno_map_paths[idx])\n",
        "\n",
        "        return image, annotation"
      ],
      "metadata": {
        "id": "2f67GpcszUp_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "training_dataset = CamVid_Dataset('/tmp/fcnn/dataset1/images_prepped_train/','/tmp/fcnn/dataset1/annotations_prepped_train/')\n",
        "validation_dataset = CamVid_Dataset('/tmp/fcnn/dataset1/images_prepped_test/','/tmp/fcnn/dataset1/annotations_prepped_test/')\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "mOKAXEHt8YEs"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a list that contains one color for each class\n",
        "colors = sns.color_palette(None, len(class_names))\n",
        "\n",
        "# print class name - normalized RGB tuple pairs\n",
        "# the tuple values will be multiplied by 255 in the helper functions later\n",
        "# to convert to the (0,0,0) to (255,255,255) RGB values you might be familiar with\n",
        "for class_name, color in zip(class_names, colors):\n",
        "  print(f'{class_name} -- {color}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2GX_KAR_2l4",
        "outputId": "44823cbe-2e18-4650-b658-00d46cdb0e99"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sky -- (0.12156862745098039, 0.4666666666666667, 0.7058823529411765)\n",
            "building -- (1.0, 0.4980392156862745, 0.054901960784313725)\n",
            "column/pole -- (0.17254901960784313, 0.6274509803921569, 0.17254901960784313)\n",
            "road -- (0.8392156862745098, 0.15294117647058825, 0.1568627450980392)\n",
            "side walk -- (0.5803921568627451, 0.403921568627451, 0.7411764705882353)\n",
            "vegetation -- (0.5490196078431373, 0.33725490196078434, 0.29411764705882354)\n",
            "traffic light -- (0.8901960784313725, 0.4666666666666667, 0.7607843137254902)\n",
            "fence -- (0.4980392156862745, 0.4980392156862745, 0.4980392156862745)\n",
            "vehicle -- (0.7372549019607844, 0.7411764705882353, 0.13333333333333333)\n",
            "pedestrian -- (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)\n",
            "byciclist -- (0.12156862745098039, 0.4666666666666667, 0.7058823529411765)\n",
            "void -- (1.0, 0.4980392156862745, 0.054901960784313725)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ySKaTRWV_3Ge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}