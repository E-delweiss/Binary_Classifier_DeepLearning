{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16-FCN8-CamVid_TF_to_PT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMu7+61b8hXHOobqvK/Wf/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Binary_Classifier_DeepLearning/blob/main/VGG16_FCN8_CamVid_TF_to_PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vbyflefu45i",
        "outputId": "635bc65e-84f6-4be8-877a-d22c80a92737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install torch-summary\n",
        "from torchsummary import summary\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset (zipped file)\n",
        "!gdown --id 0B0d9ZiqAgFkiOHR1NTJhWVJMNEU -O /tmp/fcnn-dataset.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKq36e_gvZXE",
        "outputId": "bfeb9c5f-cef7-4e96-fcc2-21e53ff7a4e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0B0d9ZiqAgFkiOHR1NTJhWVJMNEU\n",
            "To: /tmp/fcnn-dataset.zip\n",
            "100% 126M/126M [00:02<00:00, 59.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the downloaded dataset to a local directory: /tmp/fcnn\n",
        "local_zip = '/tmp/fcnn-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/fcnn')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "xzJQwHUIvbaR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show folders inside the dataset you downloaded\n",
        "!ls /tmp/fcnn/dataset1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOb1avJbyRkh",
        "outputId": "0f12f8c1-3b46-47a8-f808-e27ca2f087de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations_prepped_test   images_prepped_test\n",
            "annotations_prepped_train  images_prepped_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pixel labels in the video frames\n",
        "class_names = ['sky', 'building','column/pole', 'road', 'side walk', \n",
        "               'vegetation', 'traffic light', 'fence', 'vehicle', \n",
        "               'pedestrian', 'byciclist', 'void']"
      ],
      "metadata": {
        "id": "KkofNS4xVlY-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CamVid_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, anno_map_dir, class_names, height=224, width=224):      \n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # generates the lists of image and label map paths\n",
        "        image_file_list = os.listdir(image_dir)\n",
        "        anno_map_file_list = os.listdir(anno_map_dir)\n",
        "        self.image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "        self.anno_map_paths = [os.path.join(anno_map_dir, fname) for fname in anno_map_file_list]\n",
        "        \n",
        "        self.class_names = class_names\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def preprocess(self, img_path, anno_path):\n",
        "        # reading\n",
        "        image = torchvision.io.read_image(img_path)\n",
        "        annotation = torchvision.io.read_image(anno_path)\n",
        "\n",
        "        # resizing\n",
        "        image = torchvision.transforms.Resize((self.height, self.width))(image)\n",
        "        annotation = torchvision.transforms.Resize((self.height, self.width))(annotation).to(torch.int32)\n",
        "\n",
        "        # Reshape segmentation masks\n",
        "        stack_list = []\n",
        "        for c in range(len(self.class_names)):\n",
        "            mask = torch.eq(annotation[0, :, :], torch.as_tensor(c, dtype=torch.int32))\n",
        "            stack_list.append(mask)\n",
        "        annotation = torch.stack(stack_list, dim=2)\n",
        "        \n",
        "        # Normalizing between -1 and 1\n",
        "        image = image / 127.5\n",
        "        image -= 1\n",
        "\n",
        "        return image, annotation\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.to_list()\n",
        "\n",
        "        image, annotation = self.preprocess(self.image_paths[idx], self.anno_map_paths[idx])\n",
        "\n",
        "        return image, annotation"
      ],
      "metadata": {
        "id": "2f67GpcszUp_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "training_dataset = CamVid_Dataset('/tmp/fcnn/dataset1/images_prepped_train/','/tmp/fcnn/dataset1/annotations_prepped_train/', class_names)\n",
        "validation_dataset = CamVid_Dataset('/tmp/fcnn/dataset1/images_prepped_test/','/tmp/fcnn/dataset1/annotations_prepped_test/', class_names)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "mOKAXEHt8YEs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a list that contains one color for each class\n",
        "colors = sns.color_palette(None, len(class_names))\n",
        "\n",
        "# print class name - normalized RGB tuple pairs\n",
        "# the tuple values will be multiplied by 255 in the helper functions later\n",
        "# to convert to the (0,0,0) to (255,255,255) RGB values you might be familiar with\n",
        "for class_name, color in zip(class_names, colors):\n",
        "  print(f'{class_name} -- {color}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2GX_KAR_2l4",
        "outputId": "d3587b79-151c-4928-f1e0-dd537130bc57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sky -- (0.12156862745098039, 0.4666666666666667, 0.7058823529411765)\n",
            "building -- (1.0, 0.4980392156862745, 0.054901960784313725)\n",
            "column/pole -- (0.17254901960784313, 0.6274509803921569, 0.17254901960784313)\n",
            "road -- (0.8392156862745098, 0.15294117647058825, 0.1568627450980392)\n",
            "side walk -- (0.5803921568627451, 0.403921568627451, 0.7411764705882353)\n",
            "vegetation -- (0.5490196078431373, 0.33725490196078434, 0.29411764705882354)\n",
            "traffic light -- (0.8901960784313725, 0.4666666666666667, 0.7607843137254902)\n",
            "fence -- (0.4980392156862745, 0.4980392156862745, 0.4980392156862745)\n",
            "vehicle -- (0.7372549019607844, 0.7411764705882353, 0.13333333333333333)\n",
            "pedestrian -- (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)\n",
            "byciclist -- (0.12156862745098039, 0.4666666666666667, 0.7058823529411765)\n",
            "void -- (1.0, 0.4980392156862745, 0.054901960784313725)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization Utilities\n",
        "\n",
        "def fuse_with_pil(images):\n",
        "  '''\n",
        "  Creates a blank image and pastes input images\n",
        "\n",
        "  Args:\n",
        "    images (list of numpy arrays) - numpy array representations of the images to paste\n",
        "  \n",
        "  Returns:\n",
        "    PIL Image object containing the images\n",
        "  '''\n",
        "\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "  \n",
        "  return new_im\n",
        "\n",
        "\n",
        "def give_color_to_annotation(annotation, colors):\n",
        "  '''\n",
        "  Converts a 2-D annotation to a numpy array with shape (height, width, 3) where\n",
        "  the third axis represents the color channel. The label values are multiplied by\n",
        "  255 and placed in this axis to give color to the annotation\n",
        "\n",
        "  Args:\n",
        "    annotation (numpy array) - label map array\n",
        "  \n",
        "  Returns:\n",
        "    the annotation array with an additional color channel/axis\n",
        "  '''\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
        "  \n",
        "  for c in range(12):\n",
        "    segc = (annotation == c)\n",
        "    seg_img[:,:,0] += segc*(colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*(colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*(colors[c][2] * 255.0)\n",
        "  \n",
        "  return seg_img\n",
        "\n",
        "\n",
        "def show_predictions(image, labelmaps, titles, colors, iou_list, dice_score_list):\n",
        "  '''\n",
        "  Displays the images with the ground truth and predicted label maps\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- the input image\n",
        "    labelmaps (list of arrays) -- contains the predicted and ground truth label maps\n",
        "    titles (list of strings) -- display headings for the images to be displayed\n",
        "    iou_list (list of floats) -- the IOU values for each class\n",
        "    dice_score_list (list of floats) -- the Dice Score for each vlass\n",
        "  '''\n",
        "\n",
        "  true_img = give_color_to_annotation(labelmaps[1], colors)\n",
        "  pred_img = give_color_to_annotation(labelmaps[0], colors)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  images = np.uint8([image, pred_img, true_img])\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "  \n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\\n\".join(display_string_list) \n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(titles[idx], fontsize=12)\n",
        "    plt.imshow(im)\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation, colors):\n",
        "  '''\n",
        "  Displays the image and its annotation side by side\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- the input image\n",
        "    annotation (numpy array) -- the label map\n",
        "  '''\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann, colors)\n",
        "  \n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "  \n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset, colors, nb_display=9):\n",
        "  '''\n",
        "  Displays images and its annotations side by side\n",
        "\n",
        "  Args:\n",
        "    dataset (tf Dataset) - batch of images and annotations\n",
        "  '''\n",
        "\n",
        "  ds_image, ds_annotation = next(iter(training_dataloader))\n",
        "  idx = torch.randint(0, len(ds_image), (nb_display,))\n",
        "\n",
        "  plt.figure(figsize=(25, 15))\n",
        "  plt.title(\"Images And Annotations\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(zip(ds_image[idx], ds_annotation[idx])):\n",
        "    plt.subplot(3, 3, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy(), colors)\n"
      ],
      "metadata": {
        "id": "ySKaTRWV_3Ge"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_show_annotation(training_dataloader, colors)"
      ],
      "metadata": {
        "id": "VKzPD2gdadHs",
        "outputId": "473f7204-cdda-4ba8-9fd0-ff9276d7419f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 224), '|u1')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-e204f5d1e0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_show_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-96c9a31e0952>\u001b[0m in \u001b[0;36mlist_show_annotation\u001b[0;34m(dataset, colors, nb_display)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mshow_annotation_and_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-96c9a31e0952>\u001b[0m in \u001b[0;36mshow_annotation_and_image\u001b[0;34m(image, annotation, colors)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_img\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0mfused_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuse_with_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-96c9a31e0952>\u001b[0m in \u001b[0;36mfuse_with_pil\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mx_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpil_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mnew_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx_offset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2714\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2717\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 224), |u1"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x1080 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAEoCAYAAAAE6cViAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEvElEQVR4nO3YsW1CUQxA0f+ijAB1/v6zwBDUyQ5OHxEJpCByxTmtXbi7ktfMbADw3709+wAAuIVgAZAgWAAkCBYACYIFQIJgAZDwfs/y4XCYfd8fdAoAr+58Pn/NzPHa7K5g7fu+nU6nv7kKAH5Ya11+m3kJApAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQIJgAZAgWAAkCBYACYIFQMKamduX1/rctu3yuHMAeHEfM3O8NrgrWADwLF6CACQIFgAJggVAgmABkCBYACQIFgAJggVAgmABkCBYACR8A5obG/H/AgsqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hUtv1ly9dStt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}